{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b51467",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install python-whois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256f0ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "import urllib.parse\n",
    "import whois\n",
    "from datetime import datetime, timedelta\n",
    "import socket\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93263e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('normal.xls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a955da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df[['url']].iloc[1:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4060c36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "176b50fc-ba5a-49ed-a9d5-65726a3f7795",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "#1. 정규 표현식으로 IP 주소 형식 패턴 정의(범주X)\n",
    "ip_pattern = r\"(?:\\d{1,3}\\.){3}\\d{1,3}\"\n",
    "\n",
    "# 'IP_LIKE' 열 추가\n",
    "df['IP_LIKE'] = pd.Series(dtype=int)\n",
    "\n",
    "# URL 문자열 내 IP 주소 형식 존재 여부 확인 및 'IP_LIKE' 열 업데이트\n",
    "for idx, url in df['url'].items():\n",
    "    if re.search(ip_pattern, url):\n",
    "        df.loc[idx, 'IP_LIKE'] = 1\n",
    "    else:\n",
    "        df.loc[idx, 'IP_LIKE'] = 0\n",
    "\n",
    "df1['IP_LIKE'] = df['IP_LIKE'].iloc[1:10000].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a15f00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ac3fef04-cf80-421b-aa2e-da5d66eced8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.Checks the presence of @ in URL (HaveAt)(범주X)\n",
    "df.loc[:, 'AT'] = pd.Series(dtype=int)\n",
    "\n",
    "def HaveAt(url):\n",
    "    if \"@\" in url:\n",
    "        at = 1\n",
    "    else:\n",
    "        at = 0\n",
    "    return at\n",
    "\n",
    "# Apply the function to each URL and update 'AT' column\n",
    "df1.loc[:, 'AT'] = df1['url'].apply(HaveAt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9d0993",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bdb3a758-2eed-4da8-9721-78b0f8144ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Gives number of '/' in URL (URL_Depth)(범주X\n",
    "from urllib.parse import urlparse\n",
    "import pandas as pd\n",
    "\n",
    "# DataFrame에서 'URL_Depth' 컬럼 생성\n",
    "df1.loc[:, 'URL_Depth'] = pd.Series(dtype=int)\n",
    "\n",
    "def getDepth(url):\n",
    "    path = urlparse(url).path\n",
    "    # 슬래시로 분할하여 유효한 부분의 깊이를 계산\n",
    "    segments = [segment for segment in path.split('/') if segment]\n",
    "    depth = len(segments)\n",
    "\n",
    "    # 깊이를 -1, 0, 1로 제한\n",
    "    if depth == 0:\n",
    "        return -1\n",
    "    elif depth == 1:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "# Apply the function to each URL and update 'URL_Depth' column\n",
    "df1.loc[:, 'URL_Depth'] = df1['url'].apply(getDepth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2c6e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "476dd2f0-e6a1-44e7-9e94-954b63451698",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.Checking for redirection '//' in the url (Redirection)\n",
    "\n",
    "# 새로운 'Redirection' 열을 추가하고 int 형으로 초기화(범주 0)\n",
    "df1.loc[:, 'Redirection'] = pd.Series(dtype=int)\n",
    "\n",
    "def redirection(url):\n",
    "    pos = url.rfind('//')  # URL 내에서 마지막으로 나타나는 '//'의 위치를 찾기\n",
    "    if pos > 6:  # 위치가 6보다 크다면\n",
    "        if pos > 7:  # 위치가 7보다 크다면 (즉, 프로토콜 부분을 넘어서 존재하는 경우)\n",
    "            return 1  # 리디렉션이 의심되는 URL로 간주하여 1 반환\n",
    "        else:\n",
    "            return 0  # 그렇지 않다면 0 반환(의심)\n",
    "    else:\n",
    "        return 0  # '//'가 프로토콜 부분에만 있는 경우 0 반환(의심)\n",
    "\n",
    "# 각 URL에 대해 redirection 함수를 적용하고 'Redirection' 열을 업데이트\n",
    "df1.loc[:, 'Redirection'] = df1['url'].apply(redirection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4748d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b82b7bd3-5fff-46d8-b569-25cf1fde7b34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Is_Https\n",
       "1           45145\n",
       "0            4855\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#5.URL의 스킴이 'https'인지(범주X)\n",
    "import urllib.parse\n",
    "\n",
    "def is_https(url):\n",
    "    return 1 if urllib.parse.urlsplit(url).scheme == 'https' else 0\n",
    "\n",
    "df1.loc[:, 'Is_Https'] = df1['url'].apply(is_https)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69208ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c549fc93-dc94-46ad-9502-7d171fe99edb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TINY_URL\n",
       "0.0         47631\n",
       "1.0          2369\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "#6.1 20가지의 주요한 url 단축 서비스 패턴\n",
    "shorteningServices = r\"bit\\.ly|goo\\.gl|shorte\\.st|go2l\\.ink|x\\.co|ow\\.ly|t\\.co|tinyurl|tr\\.im|is\\.gd|cli\\.gs|\" \\\n",
    "                      r\"yfrog\\.com|migre\\.me|ff\\.im|tiny\\.cc|url4\\.eu|twit\\.ac|su\\.pr|twurl\\.nl|snipurl\\.com|\" \\\n",
    "                      r\"short\\.to|BudURL\\.com|ping\\.fm|post\\.ly|Just\\.as|bkite\\.com|snipr\\.com|fic\\.kr|loopt\\.us|\" \\\n",
    "                      r\"doiop\\.com|short\\.ie|kl\\.am|wp\\.me|rubyurl\\.com|om\\.ly|to\\.ly|bit\\.do|t\\.co|lnkd\\.in|db\\.tt|\" \\\n",
    "                      r\"qr\\.ae|adf\\.ly|goo\\.gl|bitly\\.com|cur\\.lv|tinyurl\\.com|ow\\.ly|bit\\.ly|ity\\.im|q\\.gs|is\\.gd|\" \\\n",
    "                      r\"po\\.st|bc\\.vc|twitthis\\.com|u\\.to|j\\.mp|buzurl\\.com|cutt\\.us|u\\.bb|yourls\\.org|x\\.co|\" \\\n",
    "                      r\"prettylinkpro\\.com|scrnch\\.me|filoops\\.info|vzturl\\.com|qr\\.net|1url\\.com|tweez\\.me|v\\.gd|\" \\\n",
    "                      r\"tr\\.im|link\\.zip\\.net|buff\\.ly|rb\\.gy|rebrand\\.ly|short\\.cm|clk\\.im|cutt\\.ly|t2m\\.io|bl\\.ink|\" \\\n",
    "                      r\"tiny\\.cc\"    #단축서비스 패턴 10개 추가함\n",
    "\n",
    "df1.loc[:, 'TINY_URL'] = pd.Series(dtype=int)\n",
    "\n",
    "#6.2 url에 단축서비스가 포함되어있는 지 확인(범주X)\n",
    "def tinyURL(url):\n",
    "    match=re.search(shorteningServices,url) #정규 표현식을 사용하여 입력된 url에서 단축서비스 패턴을 찾음\n",
    "    if match: #입력된 url에 단축서비스 패턴이 있으면 1을 리턴, 아니면 0을 리턴\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Apply the function to each URL and update 'TINY_URL' column\n",
    "df1.loc[:, 'TINY_URL'] = df1['url'].apply(tinyURL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae77ae12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fadce5-d838-41b5-8129-9f0caf33626f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7. -이 있는지 (범주X)\n",
    "def check_Hyphen(domain):\n",
    "    if '-' in domain:\n",
    "        return 1 \n",
    "    else:\n",
    "        return 0 \n",
    "\n",
    "df1['Check_Hyphen'] = df1['url'].apply(check_Hyphen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3684680",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63c3040-18b1-4660-b720-eff1faff236a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8.쿼리문자열의 개수 (범주X)\n",
    "def parse_query_string(url):\n",
    "    # URL에서 '?' 문자열이 있는지 확인하여 쿼리 문자열이 있는지 판별\n",
    "    if '?' not in url:\n",
    "        return 0\n",
    "\n",
    "    # URL에서 쿼리 문자열 추출\n",
    "    query_string = url.split('?')[-1]\n",
    "\n",
    "    # 쿼리 문자열을 '&'로 분할하여 각 쌍을 추출\n",
    "    query_pairs = query_string.split('&')\n",
    "\n",
    "    # 각 쌍을 이름과 값으로 분할하여 딕셔너리에 저장\n",
    "    params = {}\n",
    "    for pair in query_pairs:\n",
    "        if '=' in pair:\n",
    "            key, value = pair.split('=', 1)\n",
    "            params[key] = value\n",
    "        else:\n",
    "            params[pair] = None\n",
    "\n",
    "    return len(params)\n",
    "\n",
    "def categorize_query_count(count):\n",
    "    if count == 0:\n",
    "        return -1\n",
    "    elif count == 1:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def Query(url):\n",
    "    query_count = parse_query_string(url)\n",
    "    return categorize_query_count(query_count)\n",
    "\n",
    "# 쿼리 문자열의 개수를 업데이트하여 범주화\n",
    "df1['Query'] = df1['url'].apply(Query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a044b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2237ebf8-d4ac-4291-b787-eb0336bab4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#9.도메인 생성일 기준 (범주)\n",
    "import pandas as pd\n",
    "import whois\n",
    "from datetime import datetime, timedelta\n",
    "import socket\n",
    "import urllib.parse\n",
    "\n",
    "# 타임아웃 시간(초) 설정\n",
    "TIMEOUT = 3\n",
    "\n",
    "def is_domain_created(url):\n",
    "    try:\n",
    "        domain_name = urllib.parse.urlsplit(url).netloc\n",
    "        socket.setdefaulttimeout(TIMEOUT)  # 타임아웃 설정\n",
    "        domain_info = whois.whois(domain_name)\n",
    "        creation_date = domain_info.creation_date\n",
    "\n",
    "        # creation_date가 list인 경우 첫 번째 요소를 사용\n",
    "        if isinstance(creation_date, list):\n",
    "            creation_date = creation_date[0]\n",
    "\n",
    "        # creation_date가 datetime 형식이 아닌 경우 처리\n",
    "        if not isinstance(creation_date, datetime):\n",
    "            return 1  # 피싱 사이트로 간주\n",
    "\n",
    "        today = datetime.today()\n",
    "        one_years_ago = today - timedelta(days=365)\n",
    "\n",
    "        if creation_date <= one_years_ago:\n",
    "            return 0  # 1년 이상된 경우 피싱 사이트로 간주하지 않음\n",
    "        else:\n",
    "            return 1  # 1년 이하(피싱 사이트 의심)\n",
    "\n",
    "    except (whois.parser.PywhoisError, socket.timeout, Exception) as e:\n",
    "        return 1  # 오류 발생 시 1(피싱 사이트라고 간주)\n",
    "\n",
    "\n",
    "# 'Domain_Age' 열 추가\n",
    "df1['Domain_Age'] = df1['url'].apply(is_domain_created)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520c1669",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2214436e-8700-42cb-8b19-3fb54172a562",
   "metadata": {},
   "outputs": [],
   "source": [
    "#10.도메인 만료일 기준 (범주)\n",
    "import pandas as pd\n",
    "import whois\n",
    "from datetime import datetime, timedelta\n",
    "from urllib.parse import urlparse\n",
    "import socket\n",
    "import concurrent.futures\n",
    "\n",
    "# 타임아웃 시간(초) 설정\n",
    "TIMEOUT = 3\n",
    "\n",
    "# 링크에서 도메인을 추출하는 함수\n",
    "def get_domain_from_link(link):\n",
    "    parsed_uri = urlparse(link)\n",
    "    domain = '{uri.netloc}'.format(uri=parsed_uri)\n",
    "    return domain\n",
    "\n",
    "# 도메인 만료 여부를 확인하는 함수\n",
    "def domain_end(domain_name):\n",
    "    try:\n",
    "        socket.setdefaulttimeout(TIMEOUT)\n",
    "        domain_info = whois.whois(domain_name)\n",
    "        expiration_date = domain_info.expiration_date\n",
    "\n",
    "        if isinstance(expiration_date, list):\n",
    "            expiration_date = expiration_date[0]\n",
    "\n",
    "        if isinstance(expiration_date, str):\n",
    "            expiration_date = datetime.strptime(expiration_date, \"%Y-%m-%d\")\n",
    "\n",
    "        if expiration_date is None:\n",
    "            return 0\n",
    "\n",
    "        if expiration_date.tzinfo is not None:\n",
    "            expiration_date = expiration_date.replace(tzinfo=None)\n",
    "\n",
    "        today = datetime.now()\n",
    "        days_until_expiry = (expiration_date - today).days\n",
    "\n",
    "        if (days_until_expiry / 30) < 5:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    except (whois.parser.PywhoisError, socket.timeout, Exception):\n",
    "        return 1\n",
    "\n",
    "# 각 URL에 대해 도메인 만료 여부를 확인하는 함수\n",
    "def is_domain_created_within_six_months(url):\n",
    "    domain_name = get_domain_from_link(url)\n",
    "    return domain_end(domain_name)\n",
    "\n",
    "# 병렬 처리 함수\n",
    "def apply_parallel(df, func):\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        results = list(executor.map(func, df['url']))\n",
    "    return results\n",
    "\n",
    "# 병렬로 'Domain_end' 컬럼 생성 및 함수 적용\n",
    "df1['Domain_end'] = apply_parallel(df1, is_domain_created_within_six_months)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da881b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60922df0-74e3-40c5-9e06-6c8ceb7991de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#11. Mouse_over (범주X)\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def check_mouseover(html_content):\n",
    "    # BeautifulSoup을 사용하여 HTML 파싱\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    # onmouseover 속성을 가진 태그를 찾음\n",
    "    if soup.find(attrs={\"onmouseover\": True}):\n",
    "        return 1  # 마우스 오버 발생\n",
    "    else:\n",
    "        return 0  # 마우스 오버 발생 X\n",
    "\n",
    "def check_url(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)  # 타임아웃을 5초로 설정\n",
    "        response.raise_for_status()  # HTTP 에러가 발생하면 예외 발생\n",
    "        response_text = response.text\n",
    "        result = check_mouseover(response_text)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        result = -1  # 타임아웃 또는 다른 요청 예외 발생 시 -1을 반환\n",
    "    return result\n",
    "\n",
    "\n",
    "# 'url' 컬럼의 각 URL에 대해 Mouseover 여부를 확인하여 'Mouseover' 컬럼에 결과를 저장\n",
    "df1['Mouseover'] = df1['url'].apply(check_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268d3381",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f11e986-bb6b-4a6d-ba5f-a061a2306898",
   "metadata": {},
   "outputs": [],
   "source": [
    "#12. Web_Forwards(범주)\n",
    "import requests\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# 웹 포워딩 체크 함수\n",
    "def web_forwards(url, timeout=5):\n",
    "    try:\n",
    "        respon = requests.get(url, allow_redirects=True, timeout=timeout)\n",
    "    except requests.RequestException:\n",
    "        return 1  # 요청 중 오류가 발생하면 피싱 사이트로 간주\n",
    "\n",
    "    if not respon.history:\n",
    "        return 0  # 리디렉션이 전혀 없는 경우 의심 사이트\n",
    "\n",
    "    # 리디렉션 횟수가 2회 이하인 경우 의심 사이트로 간주\n",
    "    if len(respon.history) <= 2:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1  # 리디렉션이 3회 이상인 경우 피싱 사이트로 간주\n",
    "\n",
    "# 병렬 처리를 사용하여 각 URL에 대해 web_forwards 함수 실행\n",
    "def parallel_apply(urls, func, max_workers=5):\n",
    "    results = []\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_url = {executor.submit(func, url): url for url in urls}\n",
    "        for future in as_completed(future_to_url):\n",
    "            url = future_to_url[future]\n",
    "            try:\n",
    "                result = future.result()\n",
    "            except Exception as exc:\n",
    "                result = 1  # 예외 발생 시 피싱 사이트로 간주\n",
    "            results.append(result)\n",
    "    return results\n",
    "\n",
    "# 'url' 컬럼의 각 URL에 대해 리디렉션 여부를 확인하여 'Web_forwards' 컬럼에 결과를 저장\n",
    "df1['Web_forwards'] = parallel_apply(df1['url'], web_forwards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e6e9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584a433d-c00c-42cc-8d04-a1a2d2c70f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#14. URL이 실제로 갖고 있는 <a> 태그의 수 (범주X)\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# 타임아웃 시간(초) 설정\n",
    "TIMEOUT = 3\n",
    "\n",
    "# URL의 <a> 태그 수를 세는 함수\n",
    "def count_hyperlinks(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=TIMEOUT)\n",
    "        response.raise_for_status()  # 요청이 성공했는지 확인\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        hyperlinks = soup.find_all('a')\n",
    "        return len(hyperlinks)\n",
    "    except (requests.RequestException, requests.Timeout):\n",
    "        return -1  # 요청 실패 시 -1 반환\n",
    "\n",
    "# <a> 태그 수를 범주화하는 함수\n",
    "def categorize_hyperlink_count(count):\n",
    "    if count == -1:\n",
    "        return -1\n",
    "    elif count == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "# 병렬 처리를 사용하여 각 URL에 대해 count_hyperlinks 함수 실행\n",
    "def parallel_apply(urls, func, max_workers=5):\n",
    "    results = []\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_url = {executor.submit(func, url): url for url in urls}\n",
    "        for future in as_completed(future_to_url):\n",
    "            url = future_to_url[future]\n",
    "            try:\n",
    "                result = future.result()\n",
    "            except Exception as exc:\n",
    "                result = -1  # 예외 발생 시 -1 반환\n",
    "            results.append(result)\n",
    "    return results\n",
    "\n",
    "# 'url' 컬럼의 각 URL에 대해 <a> 태그 수를 세어 'Hyperlinks' 컬럼에 결과를 저장\n",
    "hyperlink_counts = parallel_apply(df1['url'], count_hyperlinks)\n",
    "df1['Hyperlinks'] = [categorize_hyperlink_count(count) for count in hyperlink_counts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c782749e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd2f3a2-de47-41fd-80a1-23a8eeabf895",
   "metadata": {},
   "outputs": [],
   "source": [
    "#15. 도메인 일관성을 확인하는 함수(범주)\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import pandas as pd\n",
    "\n",
    "# 도메인 일관성 체크 함수\n",
    "def check_domain_consistency(url, timeout=3):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=timeout)\n",
    "        original_domain = urlparse(url).netloc\n",
    "        final_domain = urlparse(response.url).netloc\n",
    "        is_same_domain = 1 if original_domain == final_domain else 0\n",
    "        return is_same_domain\n",
    "    except requests.RequestException:\n",
    "        return -1\n",
    "\n",
    "# 병렬 처리를 사용하여 각 URL에 대해 check_domain_consistency 함수 실행\n",
    "def parallel_apply(urls, func, max_workers=5):\n",
    "    results = []\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_url = {executor.submit(func, url): url for url in urls}\n",
    "        for future in as_completed(future_to_url):\n",
    "            url = future_to_url[future]\n",
    "            try:\n",
    "                result = future.result()\n",
    "            except Exception as exc:\n",
    "                result = -1  # 예외 발생 시 -1 반환\n",
    "            results.append(result)\n",
    "    return results\n",
    "\n",
    "# 'url' 컬럼의 각 URL에 대해 도메인 일관성을 확인하여 'Domain_Cons' 컬럼에 결과를 저장\n",
    "df1['Domain_Cons'] = parallel_apply(df1['url'], check_domain_consistency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45074a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd1b6cc-b907-4880-89d0-ee18fe2d922a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#16. 최종 URL 길이를 반환하는 함수(범주X)\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# 예제 데이터를 큰 크기로 생성 (여기서는 예제라 간단히 합니다)\n",
    "data = {'url': ['https://www.example.com'] * 20000}  # 20000개의 URL 생성\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# DataFrame을 슬라이스\n",
    "df1 = df[['url']].iloc[    :    ]\n",
    "\n",
    "# 최종 URL 길이를 반환하는 함수\n",
    "def get_final_url_length(url):\n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "        }\n",
    "        response = requests.get(url, headers=headers, timeout=3)\n",
    "        final_url_length = len(response.url)\n",
    "        return final_url_length\n",
    "    except requests.RequestException:\n",
    "        return -1  # 에러 발생 시 -1 반환\n",
    "\n",
    "# 각 URL의 최종 URL 길이를 계산하여 새로운 열에 추가\n",
    "df1['URL_length'] = df1['url'].apply(get_final_url_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574c7fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa8db4c-e389-411a-b9d7-d4811e2fcee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#17. HTTP 상태 코드 리스트를 반환하는 함수(범주X)\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# 예제 데이터를 큰 크기로 생성 (여기서는 예제라 간단히 합니다)\n",
    "data = {'url': ['https://www.example.com', 'https://www.google.com', \n",
    "                'https://www.nonexistentwebsite12345.com', 'https://httpstat.us/404', 'https://httpstat.us/500'] * 2000}  # 10000개의 URL 생성\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# DataFrame을 슬라이스\n",
    "df1 = df[['url']].iloc[    :    ]\n",
    "\n",
    "# HTTP 상태 코드를 반환하는 함수\n",
    "def get_http_status_codes(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=3)\n",
    "        return response.status_code\n",
    "    except requests.RequestException:\n",
    "        return 0  # 반환을 못하는 경우\n",
    "\n",
    "# 각 URL의 HTTP 상태 코드를 계산하여 새로운 열에 추가\n",
    "df1['HTTP_Code'] = df1['url'].apply(get_http_status_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de063cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c076b8-aa8c-41fd-a06a-d7a95147e7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#18. HTTP 상태 코드 리스트를 반환하는 함수(범주)\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# 예제 데이터를 큰 크기로 생성 (여기서는 예제라 간단히 합니다)\n",
    "data = {'url': ['https://www.example.com', 'https://www.google.com', \n",
    "                'https://www.nonexistentwebsite12345.com', 'https://httpstat.us/404', 'https://httpstat.us/500'] * 4000}  # 20000개의 URL 생성\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# DataFrame을 슬라이스\n",
    "df1 = df[['url']].iloc[    :    ]\n",
    "\n",
    "# HTTP 상태 코드를 반환하는 함수\n",
    "def get_http_status_codes(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=3)\n",
    "        # 최종 응답의 상태 코드만 반환\n",
    "        return response.status_code\n",
    "    except requests.RequestException:\n",
    "        return None  # 에러 발생 시 None 반환\n",
    "\n",
    "# 주어진 URL에 대해 피처를 추출하는 함수\n",
    "def phishing_likelihood(url):\n",
    "    try:\n",
    "        # HTTP 상태 코드를 가져옴\n",
    "        http_status_codes = get_http_status_codes(url)\n",
    "        if http_status_codes is None or http_status_codes >= 400:\n",
    "            return 0  # 접속 불가능\n",
    "        else:\n",
    "            return 1  # 접속 가능\n",
    "    except Exception:\n",
    "        return 0  # 에러 발생 시 0 반환\n",
    "\n",
    "# 각 URL의 피싱 가능성을 계산하여 새로운 열에 추가\n",
    "df1['Phishing_Likelihood'] = df1['url'].apply(phishing_likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69fb67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4683f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#19. HTTP 상태 코드 리스트를 반환하는 함수(범주)\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# 예제 데이터를 큰 크기로 생성 (여기서는 예제라 간단히 합니다)\n",
    "data = {'url': ['https://www.example.com', 'https://www.google.com', \n",
    "                'https://www.nonexistentwebsite12345.com', 'https://httpstat.us/404', 'https://httpstat.us/500'] * 4000}  # 20000개의 URL 생성\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# DataFrame을 슬라이스\n",
    "df1 = df[['url']].iloc[    :    ]\n",
    "\n",
    "# HTTP 상태 코드를 반환하는 함수\n",
    "def get_http_status_codes(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=3)\n",
    "        # 최종 응답의 상태 코드만 반환\n",
    "        return response.status_code\n",
    "    except requests.RequestException as e:\n",
    "        return None  # 에러 발생 시 None 반환\n",
    "\n",
    "# 주어진 URL에 대해 피처를 추출하는 함수\n",
    "def phishing_likelihood(url):\n",
    "    try:\n",
    "        # HTTP 상태 코드를 가져옴\n",
    "        http_status_code = get_http_status_codes(url)\n",
    "        if http_status_code is None or http_status_code >= 300:\n",
    "            return 1  # 접속 불가능 또는 비정상 응답은 피싱 가능성 있음\n",
    "        else:\n",
    "            return 0  # 접속 가능\n",
    "    except Exception as e:\n",
    "        return 1  # 에러 발생 시 1 반환\n",
    "\n",
    "# 각 URL의 피싱 가능성을 계산하여 새로운 열에 추가\n",
    "df1['Phishing_Likelihood'] = df1['url'].apply(phishing_likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61395acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
