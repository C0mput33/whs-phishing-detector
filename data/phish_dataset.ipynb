{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "176b50fc-ba5a-49ed-a9d5-65726a3f7795",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "#1. 정규 표현식으로 IP 주소 형식 패턴 정의(범주X)\n",
    "ip_pattern = r\"(?:\\d{1,3}\\.){3}\\d{1,3}\"\n",
    "\n",
    "# 'IP_LIKE' 열 추가\n",
    "df['IP_LIKE'] = pd.Series(dtype=int)\n",
    "\n",
    "# URL 문자열 내 IP 주소 형식 존재 여부 확인 및 'IP_LIKE' 열 업데이트\n",
    "for idx, url in df['url'].items():\n",
    "    if re.search(ip_pattern, url):\n",
    "        df.loc[idx, 'IP_LIKE'] = 1\n",
    "    else:\n",
    "        df.loc[idx, 'IP_LIKE'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ac3fef04-cf80-421b-aa2e-da5d66eced8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.Checks the presence of @ in URL (HaveAt)(범주X)\n",
    "df.loc[:, 'AT'] = pd.Series(dtype=int)\n",
    "\n",
    "def HaveAt(url):\n",
    "    if \"@\" in url:\n",
    "        at = 1\n",
    "    else:\n",
    "        at = 0\n",
    "    return at\n",
    "\n",
    "# Apply the function to each URL and update 'AT' column\n",
    "df.loc[:, 'AT'] = df['url'].apply(HaveAt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bdb3a758-2eed-4da8-9721-78b0f8144ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "\n",
    "#3. Gives number of '/' in URL (URL_Depth)(범주X)\n",
    "df.loc[:, 'URL_Depth'] = pd.Series(dtype=int)\n",
    "\n",
    "def getDepth(url):\n",
    "  s = urlparse(url).path.split('/')\n",
    "  depth = 0\n",
    "  for j in range(len(s)):\n",
    "    if len(s[j]) != 0:\n",
    "      depth = depth+1\n",
    "  return depth\n",
    "    \n",
    "# Apply the function to each URL and update 'URL_Depth' column\n",
    "df.loc[:, 'URL_Depth'] = df['url'].apply(getDepth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "476dd2f0-e6a1-44e7-9e94-954b63451698",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.Checking for redirection '//' in the url (Redirection)\n",
    "\n",
    "# 새로운 'Redirection' 열을 추가하고 int 형으로 초기화(범주 0)\n",
    "df.loc[:, 'Redirection'] = pd.Series(dtype=int)\n",
    "\n",
    "def redirection(url):\n",
    "    pos = url.rfind('//')  # URL 내에서 마지막으로 나타나는 '//'의 위치를 찾기\n",
    "    if pos > 6:  # 위치가 6보다 크다면\n",
    "        if pos > 7:  # 위치가 7보다 크다면 (즉, 프로토콜 부분을 넘어서 존재하는 경우)\n",
    "            return 1  # 리디렉션이 의심되는 URL로 간주하여 1 반환\n",
    "        else:\n",
    "            return 0  # 그렇지 않다면 0 반환(의심)\n",
    "    else:\n",
    "        return 0  # '//'가 프로토콜 부분에만 있는 경우 0 반환(의심)\n",
    "\n",
    "# 각 URL에 대해 redirection 함수를 적용하고 'Redirection' 열을 업데이트\n",
    "df.loc[:, 'Redirection'] = df['url'].apply(redirection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b82b7bd3-5fff-46d8-b569-25cf1fde7b34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Is_Https\n",
       "1           45145\n",
       "0            4855\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#5.URL의 스킴이 'https'인지(범주X)\n",
    "import urllib.parse\n",
    "\n",
    "def is_https(url):\n",
    "    return 1 if urllib.parse.urlsplit(url).scheme == 'https' else 0\n",
    "\n",
    "df.loc[:, 'Is_Https'] = df['url'].apply(is_https)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c549fc93-dc94-46ad-9502-7d171fe99edb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TINY_URL\n",
       "0.0         47631\n",
       "1.0          2369\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "#6.1 20가지의 주요한 url 단축 서비스 패턴\n",
    "shorteningServices = r\"bit\\.ly|goo\\.gl|shorte\\.st|go2l\\.ink|x\\.co|ow\\.ly|t\\.co|tinyurl|tr\\.im|is\\.gd|cli\\.gs|\" \\\n",
    "                      r\"yfrog\\.com|migre\\.me|ff\\.im|tiny\\.cc|url4\\.eu|twit\\.ac|su\\.pr|twurl\\.nl|snipurl\\.com|\" \\\n",
    "                      r\"short\\.to|BudURL\\.com|ping\\.fm|post\\.ly|Just\\.as|bkite\\.com|snipr\\.com|fic\\.kr|loopt\\.us|\" \\\n",
    "                      r\"doiop\\.com|short\\.ie|kl\\.am|wp\\.me|rubyurl\\.com|om\\.ly|to\\.ly|bit\\.do|t\\.co|lnkd\\.in|db\\.tt|\" \\\n",
    "                      r\"qr\\.ae|adf\\.ly|goo\\.gl|bitly\\.com|cur\\.lv|tinyurl\\.com|ow\\.ly|bit\\.ly|ity\\.im|q\\.gs|is\\.gd|\" \\\n",
    "                      r\"po\\.st|bc\\.vc|twitthis\\.com|u\\.to|j\\.mp|buzurl\\.com|cutt\\.us|u\\.bb|yourls\\.org|x\\.co|\" \\\n",
    "                      r\"prettylinkpro\\.com|scrnch\\.me|filoops\\.info|vzturl\\.com|qr\\.net|1url\\.com|tweez\\.me|v\\.gd|\" \\\n",
    "                      r\"tr\\.im|link\\.zip\\.net|buff\\.ly|rb\\.gy|rebrand\\.ly|short\\.cm|clk\\.im|cutt\\.ly|t2m\\.io|bl\\.ink|\" \\\n",
    "                      r\"tiny\\.cc\"    #단축서비스 패턴 10개 추가함\n",
    "\n",
    "df.loc[:, 'TINY_URL'] = pd.Series(dtype=int)\n",
    "\n",
    "#6.2 url에 단축서비스가 포함되어있는 지 확인(범주X)\n",
    "def tinyURL(url):\n",
    "    match=re.search(shorteningServices,url) #정규 표현식을 사용하여 입력된 url에서 단축서비스 패턴을 찾음\n",
    "    if match: #입력된 url에 단축서비스 패턴이 있으면 1을 리턴, 아니면 0을 리턴\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Apply the function to each URL and update 'TINY_URL' column\n",
    "df.loc[:, 'TINY_URL'] = df['url'].apply(tinyURL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fadce5-d838-41b5-8129-9f0caf33626f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7. -이 있는지 (범주X)\n",
    "def check_Hyphen(domain):\n",
    "    if '-' in domain:\n",
    "        return 1 \n",
    "    else:\n",
    "        return 0 \n",
    "\n",
    "data['Check_Hyphen'] = data['url'].apply(check_phishing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63c3040-18b1-4660-b720-eff1faff236a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8.쿼리문자열의 개수 (범주X)\n",
    "\n",
    "def parse_query_string(url):\n",
    "    # URL에서 '?' 문자열이 있는지 확인하여 쿼리 문자열이 있는지 판별\n",
    "    if '?' not in url:\n",
    "        return 0\n",
    "    \n",
    "    # URL에서 쿼리 문자열 추출\n",
    "    query_string = url.split('?')[-1]\n",
    "    \n",
    "    # 쿼리 문자열을 '&'로 분할하여 각 쌍을 추출\n",
    "    query_pairs = query_string.split('&')\n",
    "    \n",
    "    # 각 쌍을 이름과 값으로 분할하여 딕셔너리에 저장\n",
    "    params = {}\n",
    "    for pair in query_pairs:\n",
    "        key, value = pair.split('=')\n",
    "        params[key] = value\n",
    "    \n",
    "    return params\n",
    "\n",
    "\n",
    "def Query(url):\n",
    "    query_params = parse_query_string(url)\n",
    "    if query_params is not None:\n",
    "        return len(query_params)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# 쿼리 문자열의 개수를 업데이트\n",
    "df['Query'] = df['url'].apply(Query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2237ebf8-d4ac-4291-b787-eb0336bab4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#9.도메인 생성일 기준 (범주)\n",
    "import whois\n",
    "from datetime import datetime, timedelta\n",
    "import socket\n",
    "\n",
    "# 타임아웃 시간(초) 설정\n",
    "TIMEOUT = 5\n",
    "\n",
    "def is_domain_created(url):\n",
    "    try:\n",
    "        domain_name = urllib.parse.urlsplit(url).netloc\n",
    "        socket.setdefaulttimeout(TIMEOUT)  # 타임아웃 설정\n",
    "        domain_info = whois.whois(domain_name)\n",
    "        creation_date = domain_info.creation_date\n",
    "        if isinstance(creation_date, list):\n",
    "            creation_date = creation_date[0]\n",
    "        today = datetime.today()\n",
    "        one_years_ago = today - timedelta(days=365)\n",
    "        two_years_ago = today - timedelta(dats=730)\n",
    "        if creation_date and creation_date >= one_years_ago and creation_date <= two_years_ago :\n",
    "            return 1 # 1년 이하(피싱 사이트)\n",
    "        else:\n",
    "            return 0  # 2년 이상(피싱 사이트 의심)\n",
    "    except (whois.parser.PywhoisError, socket.timeout) as e:\n",
    "        return 1  # 오류 발생 시 1(피싱 사이트라고 간주)\n",
    "\n",
    "\n",
    "df.loc[:, 'Domain_Age'] = pd.Series(dtype=int)\n",
    "\n",
    "# Apply the function to each URL and update 'Domain_Age' column\n",
    "df.loc[:, 'Domain_Age'] = df['url'].apply(is_domain_created)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2214436e-8700-42cb-8b19-3fb54172a562",
   "metadata": {},
   "outputs": [],
   "source": [
    "#10.도메인 만료일 기준 (범주)\n",
    "import whois\n",
    "from datetime import datetime, timedelta\n",
    "from urllib.parse import urlparse\n",
    "import socket\n",
    "# 타임아웃 시간(초) 설정\n",
    "TIMEOUT = 5\n",
    "\n",
    "# 링크에서 도메인을 추출하는 함수\n",
    "def get_domain_from_link(link):\n",
    "    # 링크를 구문 분석하여 도메인 부분을 추출\n",
    "    parsed_uri = urlparse(link)\n",
    "    domain = '{uri.netloc}'.format(uri=parsed_uri)\n",
    "    return domain\n",
    "\n",
    "# 도메인 만료 여부를 확인하는 함수\n",
    "def domain_end(domain_name):\n",
    "    try:\n",
    "        # 타임아웃 설정\n",
    "        socket.setdefaulttimeout(TIMEOUT)\n",
    "        \n",
    "        # 도메인 정보를 가져옴\n",
    "        domain_info = whois.whois(domain_name)\n",
    "        \n",
    "        # 만료일 가져옴\n",
    "        expiration_date = domain_info.expiration_date\n",
    "        \n",
    "        # 만료일이 문자열인 경우, 날짜 형식으로 변환\n",
    "        if isinstance(expiration_date, str):\n",
    "            expiration_date = datetime.strptime(expiration_date, \"%Y-%m-%d\")\n",
    "        \n",
    "        # 만료일이 없는 경우 또는 리스트 형태인 경우에는 0을 반환\n",
    "        if expiration_date is None or isinstance(expiration_date, list):\n",
    "            return 0  # 피싱 사이트 의심\n",
    "        else:\n",
    "            # 현재 날짜 가져오기\n",
    "            today = datetime.now()\n",
    "            # 만료일까지의 남은 일수를 계산\n",
    "            days_until_expiry = (expiration_date - today).days\n",
    "            # 남은 일수를 개월 수로 변환해 6개월 이내인지 확인\n",
    "            if (days_until_expiry / 30) < 6:\n",
    "                return 1  # 6개월 이내(피싱 사이트)\n",
    "            else:\n",
    "                return 0  # 6개월 이상(의심)\n",
    "    except (whois.parser.PywhoisError, socket.timeout):\n",
    "        # 오류가 발생한 경우 1을 반환(피싱 사이트라고 간주)\n",
    "        return 1\n",
    "\n",
    "# 각 URL에 대해 도메인 만료 여부를 확인하는 함수\n",
    "def is_domain_created_within_six_months(url):\n",
    "    domain_name = get_domain_from_link(url)\n",
    "    return domain_end(domain_name)\n",
    "\n",
    "\n",
    "# 'Domain_end' 컬럼 생성 및 함수 적용\n",
    "df['Domain_end'] = df['url'].apply(is_domain_created_within_six_months)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60922df0-74e3-40c5-9e06-6c8ceb7991de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#11. Mouse_over (범주X)\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def check_mouseover(html_content):\n",
    "    # BeautifulSoup을 사용하여 HTML 파싱\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    # onmouseover 속성을 가진 태그를 찾음\n",
    "    if soup.find(attrs={\"onmouseover\": True}):\n",
    "        return 1  # 마우스 오버 발생\n",
    "    else:\n",
    "        return 0  # 마우스 오버 발생 X\n",
    "\n",
    "def check_url(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)  # 타임아웃을 5초로 설정\n",
    "        response.raise_for_status()  # HTTP 에러가 발생하면 예외 발생\n",
    "        response_text = response.text\n",
    "        result = check_mouseover(response_text)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        result = -1  # 타임아웃 또는 다른 요청 예외 발생 시 -1을 반환\n",
    "    return result\n",
    "\n",
    "\n",
    "# 'url' 컬럼의 각 URL에 대해 Mouseover 여부를 확인하여 'Mouseover' 컬럼에 결과를 저장\n",
    "df['Mouseover'] = df['url'].apply(check_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f11e986-bb6b-4a6d-ba5f-a061a2306898",
   "metadata": {},
   "outputs": [],
   "source": [
    "#12. Web_Forwards(범주)\n",
    "import requests\n",
    "\n",
    "def web_forwards(url, timeout=5):\n",
    "    try:\n",
    "        respon = requests.get(url, allow_redirects=True, timeout=timeout)\n",
    "    except requests.RequestException:\n",
    "        return 1  # 요청 중 오류가 발생하면 피싱 사이트로 간주\n",
    "\n",
    "    if not respon.history:\n",
    "        return 0  # 리디렉션이 전혀 없는 경우 의심 사이트 \n",
    "\n",
    "    # 리디렉션 횟수가 2회 이하인 경우 의심 사이트로 간주\n",
    "    if len(respon.history) <= 2:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1  # 리디렉션이 3회 이상인 경우 피싱 사이트로 간주\n",
    "\n",
    "# 'url' 컬럼의 각 URL에 대해 리디렉션 여부를 확인하여 'Web_forwards' 컬럼에 결과를 저장\n",
    "df['Web_forwards'] = df['url'].apply(web_forwards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61360f5b-b8f6-45b3-adfa-8157251e5061",
   "metadata": {},
   "outputs": [],
   "source": [
    "#13.구글 검색 여부 (범주X)\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def is_indexed_in_google(url):\n",
    "    try:\n",
    "        response = requests.get(f\"https://www.google.com/search?q=site:{url}\")\n",
    "        if response.status_code == 200:\n",
    "            if \"did not match any documents\" in response.text:\n",
    "                return False\n",
    "            else:\n",
    "                return True\n",
    "        else:\n",
    "            #Failed to retrieve search results from Google\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        #error\n",
    "        return None\n",
    "        \n",
    "# 구글에 인덱싱되었는지 여부를 나타내는 새로운 열 추가\n",
    "df['Google_search'] = df['url'].apply(is_indexed_in_google)\n",
    "\n",
    "# 1과 0으로 구글 검색 여부를 인코딩하여 업데이트\n",
    "df['Google_search'] = df['Google_search'].apply(lambda x: 1 if x else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584a433d-c00c-42cc-8d04-a1a2d2c70f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#14. URL이 실제로 갖고 있는 <a> 태그의 수 (범주X)\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 타임아웃 시간(초) 설정\n",
    "TIMEOUT = 5\n",
    "\n",
    "def count_hyperlinks(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=TIMEOUT)\n",
    "        response.raise_for_status()  # 요청이 성공했는지 확인\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        hyperlinks = soup.find_all('a')\n",
    "        return len(hyperlinks)\n",
    "    except (requests.RequestException, requests.Timeout):\n",
    "        return -1  # 요청 실패 시 None 반환\n",
    "\n",
    "\n",
    "# 하이퍼링크의 총 개수를 업데이트\n",
    "df['Hyperlinks'] = df['url'].apply(count_hyperlinks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd2f3a2-de47-41fd-80a1-23a8eeabf895",
   "metadata": {},
   "outputs": [],
   "source": [
    "#15. 도메인 일관성을 확인하는 함수(범주) \n",
    "def check_domain_consistency(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        original_domain = urlparse(url).netloc\n",
    "        final_domain = urlparse(response.url).netloc\n",
    "        is_same_domain = 1 if original_domain == final_domain else 0\n",
    "        return is_same_domain\n",
    "    except requests.RequestException as e:\n",
    "        return 0\n",
    "\n",
    "df['Domain_Cons'] = df['url'].apply(check_domain_consistency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd1b6cc-b907-4880-89d0-ee18fe2d922a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#16. 최종 URL 길이를 반환하는 함수(범주X)\n",
    "def get_final_url_length(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        final_url_length = len(response.url)\n",
    "        return final_url_length\n",
    "    except requests.RequestException as e:\n",
    "        return -1 #error\n",
    "        \n",
    "df['URL_length'] = df['url'].apply(get_final_url_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa8db4c-e389-411a-b9d7-d4811e2fcee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#17. HTTP 상태 코드 리스트를 반환하는 함수(범주X)\n",
    "def get_http_status_codes(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        # 최종 응답의 상태 코드만 반환\n",
    "        return response.status_code\n",
    "        \n",
    "    except requests.RequestException as e:\n",
    "        return 0 #반환을 못하는 경우 \n",
    "\n",
    "df['\bHTTP_Code'] = df['url'].apply(get_http_status_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c076b8-aa8c-41fd-a06a-d7a95147e7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#18. HTTP 상태 코드 리스트를 반환하는 함수(범주)\n",
    "def get_http_status_codes(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "\b        # 최종 응답의 상태 코드만 반환\n",
    "        return response.status_code\n",
    "    except requests.RequestException as e:\n",
    "        return None #error\n",
    "\n",
    "# 주어진 URL에 대해 피처를 추출하는 함수\n",
    "def phishing_likelihood(url):\n",
    "    try:\n",
    "        # HTTP 상태 코드를 가져옴\n",
    "        http_status_codes = get_http_status_codes(url)\n",
    "        if http_status_codes is None:\n",
    "            return 1 #error(피싱 사이트라고 간주)\n",
    "        \n",
    "        # 피싱 사이트일 가능성을 나타내는 피처 초기화\n",
    "        phishing_likelihood = 0\n",
    "        \n",
    "        # 피싱 사이트일 가능성을 판단하여 피처 설정\n",
    "        if any(code >= 300 for code in http_status_codes):\n",
    "            phishing_likelihood = 1\n",
    "        else: \n",
    "            return 0 #의심 \n",
    "\n",
    "    except Exception as e:\n",
    "        return 1 #error\n",
    "\n",
    "df['\bHTTP_Status'] = df['url'].apply(get_http_status_codes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
